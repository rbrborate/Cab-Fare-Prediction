{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Name â€“ Cab Fare Prediction \n",
    "\n",
    "Problem Statement -\n",
    "\n",
    "You are a cab rental start-up company. You have successfully run the pilot project and now want to launch your cab service across the country. You have collected the historical data from your pilot project and now have a requirement to apply analytics forfare prediction. You need to design a system that predicts the fare amount for a cab ride in the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction to run code\n",
    "# You need to set the working directory path in the following code\n",
    "# And the working directory that you have set must have train_cab and test data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from geopy.distance import geodesic\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Set working directory\n",
    "os.chdir(\"F:\\\\data Scientist\\\\Project 2 Edwisor\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#For the given project we have provided the train and test data sets seperately. so now we will load the data sets and observe it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load given train data \n",
    "train_cab= pd.read_csv('train_cab.csv', na_values={\"pickup_datetime\":\"43\"})\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section we will observe/visualize the data and do some data cleaning operations on both train and test data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see first five rows of train data\n",
    "train_cab.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the shape of train data\n",
    "train_cab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical measures of data\n",
    "train_cab.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see the train_cab data statistical measures in the above table.\n",
    "-we know that latitude ranges from -90 to +90 and longitude ranges from -180 to +180.\n",
    "-But in above train_cab data table pickup_latitude variable maximum value is above 90 so we need to remove those values. All other values of latitide and longitude are within range.\n",
    "-Also passenger_count variable contains minimum value as'0.0' and maximum value as '5345.0'and its data type is float. So we also need to solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of train data variables\n",
    "train_cab.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-We can see that fare_amount is in 'object' data type so it need to be converted to 'numeric'also  pickup_datetime vatiable is a 'timestamp varibale' so it is also need to be converted to 'date_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert fare_amount varible from \"object\" data type to \"numeric\"\n",
    "train_cab[\"fare_amount\"] = pd.to_numeric(train_cab[\"fare_amount\"],errors = \"coerce\")\n",
    "\n",
    "# Now convert pickup_datetime variable to date_time data type in train data\n",
    "train_cab['pickup_datetime']=pd.to_datetime(train_cab['pickup_datetime'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load given \n",
    "test_cab=pd.read_csv('test.csv')\n",
    "# To see first five rows of test data\n",
    "test_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the shape of test data\n",
    "test_cab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical measures of data\n",
    "test_cab.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of test data variables\n",
    "test_cab.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In test data also we need to convert the pickup_datetime varibale to 'date_time' as it is related to time and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert pickup_datetime variable to date_time data type in train data\n",
    "test_cab['pickup_datetime']=pd.to_datetime(test_cab['pickup_datetime'],errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see some visualizations to understand the train_cab data in better way\n",
    "we will plot histogram of fare_amount to see the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab['passenger_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab['fare_amount'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-By domain knowledge of this project we can say that fare_amount should not be '0' or negative. \n",
    "\n",
    "-we can see above the values beyond 453.00 are very high which is not practicaly possible. so we need to remove the observations having values more than '453'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check that is there any values below '1' in fare_amount and 'negative'\n",
    "print('values below 1=''={}'.format(sum(train_cab['fare_amount']<1)))\n",
    "print('values above 453=''={}'.format(sum(train_cab['fare_amount']>453)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab[train_cab['fare_amount']<1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop all '7' observations which are below  '1' and above '453'\n",
    "train_cab = train_cab.drop(train_cab[train_cab['fare_amount']<1].index, axis=0)\n",
    "train_cab = train_cab.drop(train_cab[train_cab['fare_amount']>453].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot box plot of passenger_count variable\n",
    "plt.figure(figsize=(20,5)) \n",
    "plt.xlim(0,100)\n",
    "sns.boxplot(x=train_cab['passenger_count'],data=train_cab,orient='h')\n",
    "plt.title('Boxplot of passenger_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We will assume maximum 6 passengers could travel in one cab\n",
    "-From the above plot we can see that passenger_count in train_cab data has some values more than '6' which is not possible practically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab[\"passenger_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see above passenger_count is having minimum value as '0' and maximum as '5345' which is not practical. There should be at least one passenger and maximum six. so we will remove the observations who have passenger_count more than '6' and less than '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove passenger_count above '6' and below '1'  \n",
    "train_cab = train_cab.drop(train_cab[train_cab['passenger_count']>6].index, axis=0)\n",
    "train_cab = train_cab.drop(train_cab[train_cab['passenger_count']<1].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck\n",
    "sum(train_cab['passenger_count']<1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab[\"passenger_count\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-As we can see the unique values of passenger_count varibale. \n",
    "-It contains 'NA' values that is missing values so we will deal with it in missing value analysis.\n",
    "-It is having 1.3 as a unique value, But it is not practical as passenger can not be 1.3 \n",
    "-so we will also remove the obersvations with value 1.3.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the observations in passenger_count with 1.3 value.\n",
    "train_cab = train_cab.drop(train_cab[train_cab['passenger_count']==1.3].index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Now passenger_count will have unique values 1.0,2.0,3.0,4.0,5.0,6.0 and 'NA'\n",
    "-It contains float values.\n",
    "-We will convert the variable in proper data type and category after the missing value analyis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the above values for test_cab data\n",
    "test_cab[\"passenger_count\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we can see that test_cab data does not contain outliers like train data.So we have completed the processing of passenger_count varibale of both train and test data.\n",
    "\n",
    "-Now in next step we will look into longitude and latitude variables\n",
    "-We already observed in train_cab data that pickup_latitude is above 90. \n",
    "-so we will reomove those observations who are outside the limit because we can not impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab[train_cab['pickup_latitude']>90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab = train_cab.drop((train_cab[train_cab['pickup_latitude']>90]).index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us check the all 4 latitude and longitude variables for presence of '0' value\n",
    "location_var= ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']\n",
    "for i in location_var:\n",
    "    print('Zero value count in',i,'={}'.format(sum(train_cab[i]==0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see all the four varibales contains '0' values so we will remove all those observations which contains '0' value from that respective varibales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations with '0' value\n",
    "for i in location_var:\n",
    "    train_cab = train_cab.drop((train_cab[train_cab[i]==0]).index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value analysis-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this step we will find the variables with missing values. If missing values are present then we will impute them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with missing values\n",
    "\n",
    "missing_val= pd.DataFrame(train_cab.isnull().sum())\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the inde of rows\n",
    "missing_val = missing_val.reset_index()\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see we have missing values in three variables fare_amount,pickup_datetime,passenger_count so we need to impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets rename the variables\n",
    "missing_val=missing_val.rename(columns= {\"index\":\"Variables\", 0:\"Missing_Percentage\"})\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of missing values\n",
    "missing_val[\"Missing_Percentage\"]= (missing_val[\"Missing_Percentage\"]/len(train_cab))*100\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets sort the Missing_percentage in descending order\n",
    "missing_val=missing_val.sort_values(\"Missing_Percentage\", ascending=False).reset_index(drop=True)\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to Impute missing values is as follows\n",
    "# 1) Select any random observation from data having its value and equal it with \"NA\"\n",
    "# 2) Now impute that value by using mean, mode, median \n",
    "# 3) Compare the value imputed by above methods with actual value.\n",
    "# 4) select the method which will give more accurate result\n",
    "# 5) Now choose that method and find all missing values in that variable.\n",
    "# 6) Repeat above steps t0 impute all missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will calculate missing values for fare_amount variable.\n",
    "# we will use mean, median  \n",
    "# Mode is not useful as it is of numeric data type value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets selecet any random observation\n",
    "train_cab['fare_amount'].loc[7230]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fare_amount location 7230 value\n",
    "# Actual value-6.9\n",
    "# Mean-11.36\n",
    "# Median-8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make this value equal to \"NA\"\n",
    "train_cab['fare_amount'].loc[7230]=np.nan\n",
    "train_cab['fare_amount'].loc[7230]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute with mean\n",
    "train_cab['fare_amount']=train_cab['fare_amount'].fillna(train_cab['fare_amount'].mean())\n",
    "train_cab['fare_amount'].loc[7230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now again make that value as equal to 'NA'\n",
    "train_cab['fare_amount'].loc[7230]=np.nan\n",
    "train_cab['fare_amount'].loc[7230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute with median\n",
    "train_cab['fare_amount']=train_cab['fare_amount'].fillna(train_cab['fare_amount'].median())\n",
    "train_cab['fare_amount'].loc[7230]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#we can see median is giving better result than mean so we will calculate all missing values using median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab['fare_amount']= train_cab['fare_amount'].fillna(train_cab['fare_amount'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets calculate missing values for passenger count variable\n",
    "# we will use mode method for imputing missing values\n",
    "\n",
    "# lets selecet any random observation\n",
    "train_cab['passenger_count'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passenger_count location '2' \n",
    "# Actual value-2.0\n",
    "# mode= 1.0\n",
    "# mean= 1.64\n",
    "# median= 1.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make this value equal to \"NA\"\n",
    "train_cab['passenger_count'].loc[2]=np.nan\n",
    "train_cab['passenger_count'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute with mode\n",
    "train_cab['passenger_count']= train_cab['passenger_count'].fillna(train_cab['passenger_count'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab['passenger_count'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now again make that value as equal to 'NA'\n",
    "train_cab['passenger_count'].loc[2]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute with mean\n",
    "train_cab['passenger_count']=train_cab['passenger_count'].fillna(train_cab['passenger_count'].mean())\n",
    "train_cab['passenger_count'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now again make that value as equal to 'NA'\n",
    "train_cab['passenger_count'].loc[2]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute with median\n",
    "train_cab['passenger_count']=train_cab['passenger_count'].fillna(train_cab['passenger_count'].median())\n",
    "train_cab['passenger_count'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see mean and median are giving good results than mode. so we will freeze mean method.\n",
    "# Also the value is of float data type but passenger_count can not be 1.64 \n",
    "# So we will convert passenger_count to int data type and make round off of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets impute all missing value with mean \n",
    "train_cab['passenger_count']=train_cab['passenger_count'].fillna(train_cab['passenger_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the details of passenger_count variable\n",
    "train_cab['passenger_count'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets round off the values \n",
    "train_cab['passenger_count'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab['passenger_count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that there is missing value in pickup_datetime variable.\n",
    "# which is only one missing value, so we will remove that observation instead of imputing it \n",
    "train_cab = train_cab.drop(train_cab[train_cab['pickup_datetime'].isnull()].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val= pd.DataFrame(train_cab.isnull().sum())\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cab.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-SO WE HAVE IMPUTED ALL THE MISSING VALUES. OUR BOTH DATA ARE FREE FROM MISSING VALUES. SO NOW WE WILL PROCEED FURTHER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Now next step after missing value analysis is actually outlier analysis.\n",
    "-But before going ot outlier analysis we will do some FEATURE ENGINEERING\n",
    "\n",
    "FEATURE ENGINEERING\n",
    "\n",
    "-If we see our variables we have date_time variable which contains information of day,month,year and time at that point. But these all are enclosed in one format there so we can not use that as it is and our ML model didnt regognise this variable so we will split that variable and create new variables like date,month,year, weekday, hour and minute.\n",
    "\n",
    "-Similarly if we see other four variables pickup_longitude,pickup_latitude,dropoff_longitude, dropoff_latitude these are nothing but the coordinates of passenger where from he is picked up and dropped by cab. \n",
    "-But we cannot use these variables also for our ML model. As we are having passenger pickup and drop coordinates we can create new variable from this that is DISTANCE variable. It will give us the distance travelled by cab during each ride.\n",
    "-Distance variable will be more important to decide Cab fare amount.\n",
    "-So we will create Distance variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING FOR TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will convert pickup_datetime variable from train_cab data to date_time format\n",
    "train_cab['pickup_datetime']=pd.to_datetime(train_cab['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will seperate pickup_datetime variable \n",
    "# create new variables like date, month, year, weekday, hour, minute\n",
    "train_cab['date']= train_cab['pickup_datetime'].dt.day\n",
    "train_cab['month']= train_cab['pickup_datetime'].dt.month\n",
    "train_cab['year']= train_cab['pickup_datetime'].dt.year\n",
    "train_cab['weekday']= train_cab['pickup_datetime'].dt.dayofweek\n",
    "train_cab['hour']= train_cab['pickup_datetime'].dt.hour\n",
    "train_cab['minute']= train_cab['pickup_datetime'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the top 5 observations of train_cab data to see newly extracted variables\n",
    "train_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create Distance variable \n",
    "# There are two formulas to calculate distance Haversine formula and Vincenty formulla\n",
    "# Haversine calculate great circle distance between latitude/longitude points assuming a spherical earth\n",
    "# Vincenty calculate geodesic distance between latitude/longitude points on ellipsoidal model of earth\n",
    "# But we know earth is not complete sphere so haversine formula will not give accurate results \n",
    "#  so we will use vincenty to create distance variable in km\n",
    "\n",
    "# lets create distance variable \n",
    "train_cab['distance']=train_cab.apply(lambda y: geodesic((y['pickup_latitude'],y['pickup_longitude']), (y['dropoff_latitude'],   y['dropoff_longitude'])).km, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the top 5 observations\n",
    "train_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the details of distance variable\n",
    "train_cab['distance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we can see that distance is having minimum value as 0.0 and maximum as 5434.77 but it is not practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets arrange the distance values in descending order to see itin detail\n",
    "train_cab['distance'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we can se values after 129.37 are increased very drasticaly so we can call it as a outlier. So we will remove those observations above 130 and having '0' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the observaions above\n",
    "sum(train_cab['distance']==0),sum(train_cab['distance']>130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove observations with '0' and more than '130' values from train data\n",
    "train_cab=train_cab.drop(train_cab[train_cab['distance']==0].index,axis=0)\n",
    "train_cab=train_cab.drop(train_cab[train_cab['distance']>130].index,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING FOR TEST DATA\n",
    "we will just repeat all the operations to test_cab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickup_datetime variable from test_cab data to date_time format\n",
    "test_cab['pickup_datetime']=pd.to_datetime(test_cab['pickup_datetime'], format='%Y-%m-%d %H:%M:%S UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will seperate pickup_datetime variable \n",
    "# create new variables like date, month, year, weekday, hour, minute in test_cab data\n",
    "test_cab['date']= test_cab['pickup_datetime'].dt.day\n",
    "test_cab['month']= test_cab['pickup_datetime'].dt.month\n",
    "test_cab['year']= test_cab['pickup_datetime'].dt.year\n",
    "test_cab['weekday']= test_cab['pickup_datetime'].dt.dayofweek\n",
    "test_cab['hour']= test_cab['pickup_datetime'].dt.hour\n",
    "test_cab['minute']= test_cab['pickup_datetime'].dt.minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create distance variable in test data \n",
    "test_cab['distance']=test_cab.apply(lambda y: geodesic((y['pickup_latitude'],y['pickup_longitude']), (y['dropoff_latitude'],   y['dropoff_longitude'])).km, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cab['distance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the observaions above\n",
    "sum(test_cab['distance']==0),sum(test_cab['distance']>130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove observations with '0' and more than '130' values from test data\n",
    "test_cab=test_cab.drop(test_cab[test_cab['distance']==0].index,axis=0)\n",
    "test_cab=test_cab.drop(test_cab[test_cab['distance']>130].index,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see shape of train_cab and test_cab data\n",
    "train_cab.shape, test_cab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make copy of train and test data sets\n",
    "train_cab_splitted= train_cab.copy()\n",
    "test_cab_splitted= test_cab.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the variables used for feature engineering\n",
    "-We have applied feature engineering techniques to both train_cab and test_cab data sets.\n",
    "-Now we will remove the variables which we have used to create new variables.\n",
    "-We have used pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude so we will remove all these from both train and test data\n",
    "-Also we will remove minute varible as it is not looking so important w.r.t. to target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets store variables to drop in one variable\n",
    "\n",
    "drop_variables= ['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'minute']\n",
    "\n",
    "# lets drop above variables from train_cab data\n",
    "train_cab= train_cab.drop(drop_variables,axis=1)\n",
    "test_cab= test_cab.drop(drop_variables,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see in data the new variables that we created are of 'float' data type \n",
    "# so we will convert all to 'int'\n",
    "train_cab['passenger_count']= train_cab['passenger_count'].astype('int64')\n",
    "train_cab['date']= train_cab['date'].astype('int64')\n",
    "train_cab['month']= train_cab['month'].astype('int64')\n",
    "train_cab['year']= train_cab['year'].astype('int64')\n",
    "train_cab['weekday']= train_cab['weekday'].astype('int64')\n",
    "train_cab['hour']= train_cab['hour'].astype('int64')\n",
    "\n",
    "\n",
    "#we can see in data the new variables that we created are of 'flaot' data type \n",
    "#so we will convert all variables to 'int' data type\n",
    "test_cab['passenger_count']= test_cab['passenger_count'].astype('int64')\n",
    "test_cab['date']= test_cab['date'].astype('int64')\n",
    "test_cab['month']= test_cab['month'].astype('int64')\n",
    "test_cab['year']= test_cab['year'].astype('int64')\n",
    "test_cab['weekday']= test_cab['weekday'].astype('int64')\n",
    "test_cab['hour']= test_cab['hour'].astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA VISUALIZATIONS\n",
    "- In this section we will visualize our data to understand it in better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see how many passengers travelling in single ride\n",
    "plt.hist(train_cab['passenger_count'],color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Single passengers are using cab service highly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the relationship between passenger count and fare amount.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x=\"passenger_count\",y=\"fare_amount\", data=train_cab,color='blue')\n",
    "plt.xlabel('No. of passengers')\n",
    "plt.ylabel('Fare_amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Cab fare is more for passenger_count of 1 and 2 as compared to other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the relationship between date and fare amount.\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x=\"date\",y=\"fare_amount\", data=train_cab,color='blue')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('Fare_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see there is no much effect of 'date' on 'fare_amount' and we are also considering 'weekday' variable which is related to 'date' \n",
    "-So we can drop date variable by observing its dependancy with other varibles in chi square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship between hour and Fare_amount\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x=\"hour\",y=\"fare_amount\", data=train_cab, color='blue')\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('fare_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-from the above plot we can say that generaly cab fare is higher after 8 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see no of cabs w.r.t. hour in day\n",
    "plt.figure(figsize=(15,7))\n",
    "train_cab.groupby(train_cab[\"hour\"])['hour'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we can see in above plot highest no of cabs are from 7 pm to 12 pm\n",
    "-Also no of cabs are less upto 5 am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the realation between fare_amount and distance variable\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(x=\"distance\",y=\"fare_amount\", data=train_cab,color='blue')\n",
    "plt.xlabel('distance')\n",
    "plt.ylabel('fare_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we can see there is a linear relatioship of distance with the fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lest see the number of cab rides w.r.t. weekday\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.countplot(x=\"weekday\", data=train_cab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-From the above plot we can say that weekday dosent have much impact on number of cab rides "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-In this step we will see the corelations between target variable and independant variables\n",
    "-Also we will perform different tests to check the relation between the depedant and indepedant variables \n",
    "-Using the results of the test finally we decide which variables should be selected and which should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even if we see the data types of all variables as 'float' and 'int'\n",
    "# we know that the variables fare_amount and distance are only numeric variables\n",
    "# Other varables expect those are having some unique values only.\n",
    "# for e.g. passenger_count has unique values 1,2,3,4,5,6.\n",
    "# They are actually categorical variables \n",
    "# So we will treat these variables as categorical variables but thier data type remains 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation analysis-  it is used to check the Correlation between the variables.\n",
    "# This can be done mostly on numeirc variables\n",
    "# lets save our numeric variables in one variable\n",
    "numeric= ['fare_amount','distance']\n",
    "          \n",
    "train_cab_corr= train_cab.loc[:,numeric]\n",
    "\n",
    "# setting the  height and width of plot\n",
    "f, ax=plt.subplots(figsize=(13,9))\n",
    "\n",
    "# correaltion matrix\n",
    "cor_matrix=train_cab_corr.corr()\n",
    "\n",
    "# plotting correlation plot\n",
    "sns.heatmap(cor_matrix,mask=np.zeros_like(cor_matrix, dtype=np.bool), \n",
    "            cmap=sns.diverging_palette(250,12,as_cmap=True),square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see that their is very high corelation between distance and fare_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-Square test-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-We have already performed anova test and we got thre result that all our categorical variables are not dependant on each other.\n",
    "-But still we will perform the Chi-square test to validate our results.\n",
    "-This test is performed to check the dependancies between cateorical variables.\n",
    "-Null hypothesis - variables are not dependant (P<0.05-Reject)\n",
    "-Alternate hypothesis- variables are dependant (P<0.05-Accept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform chi square test of independance\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "factor_data=train_cab[['passenger_count', 'date', 'weekday', 'month', 'year', 'hour']]\n",
    "for i in factor_data:\n",
    "    for j in factor_data:\n",
    "        if(i!=j):\n",
    "            chi2,p,dof, ex=chi2_contingency(pd.crosstab(train_cab[i],train_cab[j]))\n",
    "            while(p<0.05):\n",
    "                print(i,j,p)\n",
    "                          \n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-we can 'date' is correlated with 'weekday' and most of the variables.  \n",
    "-Also in data visualizations we observed that 'date' is not having much impact on 'fare_amount'\n",
    "-So we will remove 'date' variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity test using VIF(variance inflation factor)-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-VIF detects correlation between predictor variables i.e. relationship between them.\n",
    "-If two predictor variables are correlated then we can say there is presence of Multicollinearity\n",
    "-Multicollinearity affetcs the regression models so it should not present in our variables\n",
    "-So for this we do this test using VIF\n",
    "-If VIF is between 1 to 5 then we say that there is no Multicollinearity\n",
    "-If VIF>5 then there is a multicollinearity and we need to remove it or reconsider the variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create dataframe of predictor variables\n",
    "outcome, predictors = dmatrices('fare_amount ~ distance+passenger_count+date+weekday+month+year+hour',train_cab, return_type='dataframe')\n",
    "# Lets calculate VIF for each independant variables form train_cab data\n",
    "VIF = pd.DataFrame()\n",
    "VIF[\"VIF\"] = [variance_inflation_factor(predictors.values, i) for i in range(predictors.shape[1])]\n",
    "VIF[\"Predictors\"] = predictors.columns\n",
    "VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see VIF for all the predictors is within the required range i.e. from 1-5\n",
    "-So we can say that multicollinearity is not present in our independant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO AFTER PERFORMING VARIOUS TESTS ON OUR DATA FOR FEATURE SELECTION WE HAVE FOLLOWING OBSERVATIONS\n",
    "# There is no multicollinearity in our data\n",
    "# We will remove 'date' variable from both train and test data.\n",
    "# Select all other variables for our ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a copy of our data selected for Machine learning \n",
    "train_cab_selected= train_cab.copy()\n",
    "test_cab_selected= test_cab.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop 'date' variable from both the data sets\n",
    "train_cab= train_cab.drop('date', axis=1)\n",
    "test_cab= test_cab.drop('date', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SCALING-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-In this step actualy we need to do either Normalization or stadardization on numeric variables.\n",
    "-The scaling method is decided by observing histogram of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the histogram to see data distribution of fare_amount variable from train_cab data \n",
    "sns.distplot(train_cab['fare_amount'],bins='auto',color='green')\n",
    "plt.title(\"Distribution for fare_amount variable \")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot histogram for distance variable from train data.\n",
    "sns.distplot(train_cab['distance'],bins='auto',color='green')\n",
    "plt.title(\"Distribution for distance variable \")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the histogram to see data distribution distance variable from test_cab data \n",
    "\n",
    "sns.distplot(test_cab['distance'],bins='auto',color='green')\n",
    "plt.title(\"Distribution for distance variable from test data\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We can see the histogram for all the variables is left skewed.\n",
    "-That means it contains the values which will impact more on ML model.\n",
    "-As we know that Normalization is sensitive for these data type of data so we can not use normalizatoin or standardization method of scaling\n",
    "-So we will apply log tranform to both the variables to remove the effect pf skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply log tranform on numeric variables from train and test data\n",
    "train_cab['fare_amount'] = np.log1p(train_cab['fare_amount'])\n",
    "train_cab['distance'] = np.log1p(train_cab['distance'])\n",
    "test_cab['distance'] = np.log1p(test_cab['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets plot the histogram to see data distribution of fare_amount variable from train_cab data after log transform\n",
    "sns.distplot(train_cab['fare_amount'],bins='auto',color='green')\n",
    "plt.title(\"Distribution for fare_amount variable after log transform \")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cab.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-So now we have done all the data preprocessing operations on given train and test data. Our data is clean and ready to be used for ML models. So lets proceed further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLITTING OF DATA. \n",
    "# first store all predictor variables in 'x' and target variable in 'y' from train_cab data\n",
    "x=train_cab.drop(['fare_amount'],axis=1)  # predictors\n",
    "y=train_cab['fare_amount']                 # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split our train_cab data into train and test data\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check teh shape of all the data sets we have created\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MACHINE LEARNING MODEL BUILDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Before procedding to model building lets decide the error metrics\n",
    "#The error metrics that we will use are RMSLE, RMSE, R-square, Adjusted R-square, MAPE.\n",
    "#We will use RMSLE metric because it uses log transform and gives best results for the data \n",
    "#we will also use Adjusted R-square value as it is more optimized that R-square to see the performance of model\n",
    "#We will create the functions to calculate above selected error metrics for simplicity\n",
    "#we will also define a function to calculate the predicions of models for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSLE\n",
    "def rmsle(yt,yp):    # yt- y_train and yp- y_predicted\n",
    "    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in yt]))\n",
    "    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in yp]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return np.sqrt(np.mean(calc))\n",
    "# Function to calculate other error metrics\n",
    "def error_metrics(yt, yp):\n",
    "    print('r square  ', metrics.r2_score(yt,yp))\n",
    "    print('Adjusted r square:{}'.format(1 - (1-metrics.r2_score(yt, yp))*(len(yt)-1)/(len(yt)-x_train.shape[1]-1)))\n",
    "    print('MAPE:{}'.format(np.mean(np.abs((yt - yp) / yt))*100))\n",
    "    print('MSE:', metrics.mean_squared_error(yt, yp))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(yt, yp))) \n",
    "\n",
    "# Function to calculate and print the predictions of models for train and test data with its error metrics\n",
    "def output_scores(model):\n",
    "    print('###############   Error Metrics of Train data   #################')\n",
    "    print()\n",
    "    # Applying the model on train data to predict target variable\n",
    "    y_predicted = model.predict(x_train)\n",
    "    error_metrics(y_train,y_predicted)\n",
    "    print('RMSLE:',rmsle(y_train,y_predicted))\n",
    "    print()\n",
    "    print('###############   Error Metrics of Test data  #################')\n",
    "    print()\n",
    "    # Applying the model on train data to predict target variable\n",
    "    y_predicted = model.predict(x_test)\n",
    "    error_metrics(y_test,y_predicted)\n",
    "    print('RMSLE:',rmsle(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets build the model on train data\n",
    "model = sm.OLS(y_train, x_train).fit()\n",
    "\n",
    "# predict the test data\n",
    "y_predict = model.predict(x_test) \n",
    " \n",
    "# lets print model summary\n",
    "print_model = model.summary()\n",
    "print(print_model)\n",
    "\n",
    "# error metrics on train and test data\n",
    "output_scores(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model results for test data-\n",
    "-Adjusted r square for test data-0.7384\n",
    "-MAPE is 8.13 % for test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets build the lasso model \n",
    "lasso_model = Lasso(alpha=0.00021209508879201905, normalize=False, max_iter = 500)\n",
    "\n",
    "#Lets fit the lasso model on train data\n",
    "lasso_model.fit(x_train,y_train)\n",
    "\n",
    "#Lets print the coefficients for predictors\n",
    "coefficients = lasso_model.coef_\n",
    "print('coefficients of predictors:{}'.format(coefficients))\n",
    "\n",
    "# Error metrics of train and test data\n",
    "output_scores(lasso_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Model results for test data-\n",
    "-Adjusted r-square for test data-0.7605\n",
    "-MAPE is 7.66 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RIDGE REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build Ridge regression model\n",
    "ridge_reg = Ridge(alpha=0.0005,max_iter = 500)\n",
    "\n",
    "# Apply model on train data\n",
    "ridge_reg.fit(x_train,y_train)\n",
    "\n",
    "# print the coefficients of predictors\n",
    "ridge_reg_coef = ridge_reg.coef_\n",
    "print('coefficients of predictors:{}'.format(ridge_reg_coef))\n",
    "\n",
    " # Error metrics of train and test data\n",
    "output_scores(ridge_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model results-\n",
    "-Adjusted r-square for test data 0.7605\n",
    "-MAPE is 7.66 %\n",
    "-we can see reults of Lasso and ridge regression are same\n",
    "-Now we will build decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISION TREE REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-This model creates the decision tree like flow chart and gives the rules to predict the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets build the model and apply it on train data\n",
    "fit_dt=DecisionTreeRegressor(max_depth= 2).fit(x_train, y_train)\n",
    "# lets print the relative importance score for predictors\n",
    "tree_features = fit_dt.feature_importances_\n",
    "print('feature importance score of predictors:{}'.format(tree_features))\n",
    "\n",
    "# Error metrics of train and test data\n",
    "output_scores(fit_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model resultsfor test data-\n",
    "-Adjusted r-square is 0.7039\n",
    "-MAPE is 9.70 %\n",
    "-Results are not good as compared to linear regression models\n",
    "-So we need to improve accuracy of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-This is the improved version of decision tree model it uses many trees in one model to improve the accuracy.\n",
    "-It feeds error of one tree to another tree to improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets build the Random forest model \n",
    "rf_model = RandomForestRegressor(n_estimators = 70, random_state=0)\n",
    "# Aply model on train data\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "# lets print the relative importance score for predictors\n",
    "Forest_features = rf_model.feature_importances_\n",
    "print('feature importance score of predictors:{}'.format(Forest_features))\n",
    "\n",
    "# Error metrics of train and test data\n",
    "output_scores(rf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model results for test data-\n",
    "-Adjusted r-square is 0.79\n",
    "-MAPE is 7.66 %\n",
    "-Results are good as compared to linear regression models and Decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-This model is nothing but the ensemble technique which provides optimized results\n",
    "-It is implementation of gradient boosted decision trees designed for speed and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build XGboost model and apply on train data\n",
    "xgb_model = GradientBoostingRegressor(n_estimators= 70, max_depth= 2)\n",
    "\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Error metrics of train and test data\n",
    "output_scores(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Results of XGBoost model are very good as compared to all other models\n",
    "-But still we will do hyper parameter tuning of Random Forest model and XGBoost model to improve the results\n",
    "-The RandomizedSearchCV function will also do cross validation of model to get best score\n",
    "-cross validation means the train data is splited into subsets for our case its is 5 \n",
    "-So from those 5 data sets one will be test and other will be train\n",
    "-The model will take all these subset data one by one as test and calculate 5 scores\n",
    "-the best score will be average of these 5 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETER TUNING OF RF AND XGBOOST MODELS TO IMPROVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build random forest model for tuning\n",
    "RF = RandomForestRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Lets see the parameters of our current RF model\n",
    "print('Random Forest current parameters')\n",
    "pprint(RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets create Random hyperparameter grid and apply Randomized Search CV on Random Forest Model\n",
    "# lets build model again as RRF\n",
    "RRF = RandomForestRegressor(random_state = 0)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_RRF = {'n_estimators': range(80,120,10), 'max_depth': range(4,12,2)}\n",
    "\n",
    "# appply Random Search CV on model with cross validation score 5\n",
    "RRF_cv = RandomizedSearchCV(RRF, random_grid_RRF, cv=5)\n",
    "\n",
    "# lets apply model on train data\n",
    "RRF_cv.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Random Forest Parameters: {}\".format(RRF_cv.best_params_))\n",
    "print(\"Best score {}\".format(RRF_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build xgboost model for tuning\n",
    "xgb_t = GradientBoostingRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Lets see the parameters of our current xgboost model\n",
    "print('XGBoost currrent Parameters \\n')\n",
    "pprint(xgb_t.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets create Random hyperparameter grid and apply Randomized  Search CV on XGBoost model\n",
    "# lets build model again as XBG\n",
    "XGB = GradientBoostingRegressor(random_state = 0)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_XGB = {'n_estimators': range(90,120,10),'max_depth': range(1,10,1)}\n",
    "                   \n",
    "\n",
    "# Apply Random Search CV on model with cross validation score 5\n",
    "XGB_cv = RandomizedSearchCV(XGB, random_grid_XGB, cv=5)\n",
    "\n",
    "# lets apply model on train data\n",
    "XGB_cv.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned XGBoost Parameters: {}\".format(XGB_cv.best_params_))\n",
    "print(\"Best score {}\".format(XGB_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL CONCLUSIONS-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-We have built Multiple linear Regression models, Decision tree, Random forest and XGBoost models. \n",
    "-But the results i.e. r-square, Adjuested r-square and MAPE metrics for test data of Random forest and XGBoost model was good\n",
    "-So we have done parameter tuning of these models to still see if we improve results.\n",
    "-After parameter tunning we can see that XGBoost model is giving highest best score that is 0.80\n",
    "-So we will finalize XGBoost model and apply this model with tuned parameters again on train_cab data.\n",
    "-Then finaly we will use that model to predict cab fare in our given test data that is our objective of this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL REGRESSION MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply the tunned parameters and build our final XGBoost model on train_cab data.\n",
    "XGB_Final = GradientBoostingRegressor( n_estimators= 110, max_depth= 3)\n",
    "\n",
    "# Apply the model on train data.\n",
    "XGB_Final.fit(x_train,y_train)\n",
    "\n",
    "# lets create and print the important features \n",
    "XGB_Final_Features = XGB_Final.feature_importances_\n",
    "print(XGB_Final_Features)\n",
    "\n",
    "# Sorting important features in descending order\n",
    "indices = np.argsort(XGB_Final_Features)[::1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "Sorted_names = [test_cab.columns[i] for i in indices]\n",
    "\n",
    "# create and set plot size \n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add horizontal bars\n",
    "plt.barh(range(pd.DataFrame(x_train).shape[1]),XGB_Final_Features[indices],align = 'center')\n",
    "plt.yticks(range(pd.DataFrame(x_train).shape[1]), Sorted_names)\n",
    "plt.savefig('Final XGBoost Model Important Features plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"XGB_Final\" train_cab and test_cab data scores\n",
    "output_scores(XGB_Final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB_Final Model results on test data-\n",
    "-Adjusted r-square is 0.81  ## which is best from the all models that we have applied on data. and above 80\n",
    "-r square for is also 0.81   \n",
    "-MAPE is 7.1 % \n",
    "-Accuracy  is 92.9\n",
    "-RMSLE: 0.069 is also less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTION OF CAB FARE FOR GIVEN TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will predic cab fare for given test data using our XGB_Final model\n",
    "#Apply XGB_Final model on test data\n",
    "Cab_fare_test = XGB_Final.predict(test_cab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the predicted  array\n",
    "Cab_fare_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add Predicted values in our Given test data\n",
    "test_cab['predicted_fare_amount'] = Cab_fare_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output of our project to csv file test_predicted in our working directory\n",
    "# That is with predicted cab fare amount variable in given test data. \n",
    "test_cab.to_csv('test_predicted_python.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
